{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configurando a API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Setup\n",
    "1. Criar um bot através do BotFather, no aplicativo do Telegram;\n",
    "2. Pegar o número do **Token** nas configurações do bot;\n",
    "3. Adicionar o Bot em um grupo e conceder privilégio de admin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Carregando as bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulação de dados Sensíveis\n",
    "from getpass import getpass\n",
    "# Conexão com a API do Telegram\n",
    "import requests\n",
    "# Manipulação dos dados da API\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variáveis de Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token do Bot criado\n",
    "TOKEN = getpass()\n",
    "\n",
    "# URL para acessar os métodos da API\n",
    "BASE_URL = f'https://api.telegram.org/bot{TOKEN}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Testando a conexão ([Fonte](https://core.telegram.org/bots/api))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O método 'getMe' retorna informações sobre o bot criado\n",
    "response = requests.get(url = f'{BASE_URL}/getMe')\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        json.loads(response.text),\n",
    "        indent= 2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O método 'getUpdates' retorna todas as mensagens captadas pelo bot\n",
    "response = requests.get(url=f'{BASE_URL}/getUpdates')\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        json.loads(response.text),\n",
    "        indent= 2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Ingestão: Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Setup\n",
    "1. Criar um Bucket no `AWS S3` para salvar a ingestão dos dados do Telegram. Nessa etapa trabalharemos com dados no formato `.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Workflow\n",
    "\n",
    "```mermaid\n",
    "  flowchart LR;\n",
    "      A[API Telegram]-- Raw Data -->B[AWS API Gateway]-- Raw Data -->C{AWS Lambda}-- Dados Filtrados -->D[(AWS S3)];\n",
    "```\n",
    "\n",
    "* Os dados serão recolhidos pela API do Telegram e, através de um Webhook, serão redirecionados para o `AWS API Gateway`. \n",
    "\n",
    "* Uma função do `AWS Lambda` estará configurada para detectar esses dados recebidos pelo `AWS Gateway`, que realizará a filtragem inicial dos dados de interesse, bem como o particionamento pela data da mensagem. \n",
    "\n",
    "* Finalmente, os dados serão salvos na primeira camada do Data Lake, no formato `.json`.\n",
    "* Salvaremos os dados em seu formato mais inicial para possibilitar um futuro reprocessamento, caso haja necessidade. Neste projeto, teremos que adotar o modelo de processamento em `streaming`, uma vez que as mensagens coletados pela API do Telegram ficam disponíveis por apenas 24h em seus servidores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Configurando a função de ingestão no `AWS Lambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "def lambda_handler(event: dict, context: dict) -> dict:\n",
    "\n",
    "  '''\n",
    "  Recebe uma mensagens do Telegram via AWS API Gateway, verifica no\n",
    "  seu conteúdo se foi produzida em um determinado grupo e a escreve, \n",
    "  em seu formato original JSON, em um bucket do AWS S3.\n",
    "  '''\n",
    "\n",
    "  # vars de ambiente\n",
    "\n",
    "  BUCKET = os.environ['AWS_S3_BUCKET']\n",
    "  TELEGRAM_CHAT_ID = int(os.environ['TELEGRAM_CHAT_ID'])\n",
    "\n",
    "  # vars lógicas\n",
    "\n",
    "  tzinfo = timezone(offset=timedelta(hours=-3))\n",
    "  date = datetime.now(tzinfo).strftime('%Y-%m-%d')\n",
    "  timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
    "\n",
    "  filename = f'{timestamp}.json'\n",
    "\n",
    "  # código principal\n",
    "\n",
    "  client = boto3.client('s3')\n",
    "  \n",
    "  try:\n",
    "\n",
    "    message = json.loads(event[\"body\"])\n",
    "    chat_id = message[\"message\"][\"chat\"][\"id\"]\n",
    "\n",
    "    if chat_id == TELEGRAM_CHAT_ID:\n",
    "\n",
    "      with open(f\"/tmp/{filename}\", mode='w', encoding='utf8') as fp:\n",
    "        json.dump(message, fp)\n",
    "\n",
    "      client.upload_file(f'/tmp/{filename}', BUCKET, f'telegram/context_date={date}/{filename}')\n",
    "\n",
    "  except Exception as exc:\n",
    "      logging.error(msg=exc)\n",
    "      return dict(statusCode=\"500\")\n",
    "\n",
    "  else:\n",
    "      return dict(statusCode=\"200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota**: É necessário configurar as variáveis de ambiente `TELEGRAM_CHAT_ID` e `BUCKET` nas configurações do Lambda. A primeira serve para conferir se o ID do chat da mensagem que está chegando, corresponde ao chat de interesse (útil caso o mesmo bot esteja conectado em mais de um chat simultaneamente). A segunda variável informa o Bucket do S3 que os dados serão salvos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Configurando o `AWS API Gateway`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A configuração da API REST no `AWS API Gateway` é feita através da própria AWS. Será criado um método `POST` e habilitada a integração com o Lambda (`Lambda Proxy Integration`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Após ser devidamente configurada, iremos salvar a URL gerada pela API Gateway\n",
    "AWS_GATEWAY_URL = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Configurando o `Webhook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aqui utilizaremos a URL fornecida pelo API Gateway para redirecionar os dados da API do Telegram. O método getUpdates deixará de funcionar após a configuração do Webhook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"ok\": true significa que a configuração está correta\n",
    "response = requests.get(url= f'{BASE_URL}setWebhook?url={AWS_GATEWAY_URL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Checando o resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"ok\" : true significa que a configuração está correta\n",
    "response = requests.get(url= f'{BASE_URL}/getWebhookInfo')\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        json.loads(response.text),\n",
    "        indent= 2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. ETL: Enriched Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Setup\n",
    "1. Criar um Bucket no `AWS S3` para salvar o resultado do ETL. Nessa etapa trabalharemos com dados no formato `.parquet`, mantendo o padrão de particionamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Workflow\n",
    "```mermaid\n",
    "    flowchart LR\n",
    "    style B fill:#063673\n",
    "    style A fill:#088232\n",
    "    style C fill:#088232\n",
    "    \n",
    "    subgraph <b>AWS_EventBridge</b>\n",
    "    direction LR\n",
    "        B[\"AWS Lambda\"]\n",
    "        subgraph <b>ETL</b>\n",
    "        direction LR\n",
    "        B[\"AWS Lambda\"] -- <i> Dados Transformados </i> --> C[(\"Data Lake (AWS S3 - 2ª Camada)\")]\n",
    "    end\n",
    "    end\n",
    "    A[(\"Data Lake (AWS S3 - 1ª Camada)\")]--  <i>Dados Filtrados</i> --> B\n",
    "\n",
    "```\n",
    "\n",
    "* Nesta etapa, iremos configurar o `AWS EventBridge` para ativar uma Função Lambda todos os dias em um horário pré definido. Essa Função Lambda será responsável pelo ETL dos dados filtrados na primeira etapa do processo, sempre processando os dados do dia *anterior* (d-1).\n",
    "\n",
    "* Ao final do processo de ETL, os dados serão carregados em uma outra camada do Data Lake, mantendo a condição de particionamento por data, porém agora, sendo salvos no formato `Apache Parquet` e estruturados de forma tabular, graças a utilização do `PyArrow`. Este formato é ideal para a etapa que virá adiante, a etapa de **Data Analytics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Configurando a Função de ETL no `AWS Lambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import boto3\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def lambda_handler(event: dict, context: dict) -> bool:\n",
    "\n",
    "  '''\n",
    "  Diariamente é executado para compactar as diversas mensagensm, no formato\n",
    "  JSON, do dia anterior, armazenadas no bucket de dados cru, em um único \n",
    "  arquivo no formato PARQUET, armazenando-o no bucket de dados enriquecidos\n",
    "  '''\n",
    "\n",
    "  # vars de ambiente\n",
    "\n",
    "  # Bucket S3 com os dados filtrados (1ª Camada do Data Lake)\n",
    "  RAW_BUCKET = os.environ['AWS_S3_BUCKET']\n",
    "  # Bucket S3 destino onde será salvo o resultado do ETL (2ª Camada do Data Lake)\n",
    "  ENRICHED_BUCKET = os.environ['AWS_S3_ENRICHED']\n",
    "\n",
    "  # vars lógicas\n",
    "\n",
    "  # Configurando o fuso horário pro Brasil\n",
    "  tzinfo = timezone(offset=timedelta(hours=-3))\n",
    "  # Pegando a data do dia anterior\n",
    "  date = (datetime.now(tzinfo) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "  # Timestamp para nomear os arquivos\n",
    "  timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
    "\n",
    "  # código principal\n",
    "\n",
    "  table = None\n",
    "  client = boto3.client('s3')\n",
    "\n",
    "  try:\n",
    "      # Lista todos os arquivos que estão na 1ª Camada do Data Lake\n",
    "      response = client.list_objects_v2(Bucket=RAW_BUCKET, Prefix=f'telegram/context_date={date}')\n",
    "\n",
    "      # Iterando sobre cada arquivo existente\n",
    "      for content in response['Contents']:\n",
    "\n",
    "        key = content['Key']\n",
    "        # Baixa o arquivo da iteração atual e salva na pasta temporária fornecida pelo Lambda\n",
    "        client.download_file(RAW_BUCKET, key, f\"/tmp/{key.split('/')[-1]}\")\n",
    "\n",
    "        # Abre o arquivo baixado em forma de leitura (lembrando que ainda estamos trabalhando com um .json)\n",
    "        with open(f\"/tmp/{key.split('/')[-1]}\", mode='r', encoding='utf8') as fp:\n",
    "          \n",
    "          # Carrega o conteúdo do arquivo\n",
    "          data = json.load(fp)\n",
    "          # Coleta apenas o que estava contido na mensagem enviada pelo usuário do Chatbot\n",
    "          data = data[\"message\"]\n",
    "\n",
    "        # Aplica a função de data wrangling para transformar os dados\n",
    "        parsed_data = parse_data(data=data)\n",
    "\n",
    "        # Insere as informações coletadas e transformadas em uma tabela do PyArrow\n",
    "        iter_table = pa.Table.from_pydict(mapping=parsed_data)\n",
    "\n",
    "        if table:\n",
    "          # Concatena a tabela formada na iteração atual com a tabela final\n",
    "          table = pa.concat_tables([table, iter_table])\n",
    "\n",
    "        else:\n",
    "          \n",
    "          table = iter_table\n",
    "          iter_table = None\n",
    "          \n",
    "      # Ao término da iteração, salva a tabela final em formato parquet    \n",
    "      pq.write_table(table=table, where=f'/tmp/{timestamp}.parquet')\n",
    "      # Carrega a tabela final na 2ª camada do data lake, particionando por data\n",
    "      client.upload_file(f\"/tmp/{timestamp}.parquet\", ENRICHED_BUCKET, f\"telegram/context_date={date}/{timestamp}.parquet\")\n",
    "\n",
    "      return True\n",
    "  \n",
    "  # Checa e retorna caso exista algum erro\n",
    "  except Exception as exc:\n",
    "      logging.error(msg=exc)\n",
    "      return False\n",
    "  \n",
    "# Função para Data Wrangling\n",
    "def parse_data(data: dict) -> dict:\n",
    "\n",
    "  date = datetime.now().strftime('%Y-%m-%d')\n",
    "  timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "  parsed_data = dict()\n",
    "\n",
    "  for key, value in data.items():\n",
    "\n",
    "      if key == 'from':\n",
    "          for k, v in data[key].items():\n",
    "              if k in ['id', 'is_bot', 'first_name']:\n",
    "                parsed_data[f\"{key if key == 'chat' else 'user'}_{k}\"] = [v]\n",
    "\n",
    "      elif key == 'chat':\n",
    "          for k, v in data[key].items():\n",
    "              if k in ['id', 'type']:\n",
    "                parsed_data[f\"{key if key == 'chat' else 'user'}_{k}\"] = [v]\n",
    "\n",
    "      elif key in ['message_id', 'date', 'text']:\n",
    "          parsed_data[key] = [value]\n",
    "\n",
    "  if not 'text' in parsed_data.keys():\n",
    "    parsed_data['text'] = [None]\n",
    "\n",
    "  return parsed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Obs.** Como o PyArrow não é uma biblioteca vínculada ao Lambda por padrão, é necessário a criação de uma Layer para utilização do `AWSSDKPandas-Python38` (este projeto está sendo desenvolvido na runtime **Python 3.8** )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Criando um *scheduler* no `AWS EventBridge`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aqui configuramos uma \"regra\" no EventBridge para executar a Função Lambda de ETL em um horário específico do dia. A única configuração necessária é a expressão *cron* correta que reflete o horário desejado, tendo em mente a diferença de **fuso horário**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Setup e Visão Geral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Workflow\n",
    "\n",
    "\n",
    "```mermaid\n",
    "    flowchart LR\n",
    "    subgraph AWS_Athena\n",
    "    style A fill:#088232\n",
    "    direction LR\n",
    "        A[(\"Data Lake (2ª Camada)\")]-- Query ---> B[\"Insight\"]\n",
    "        A[(\"Data Lake (2ª Camada)\")]-- Query ---> B[\"Insight\"]\n",
    "        A[(\"Data Lake (2ª Camada)\")]-- Query ---> B[\"Insight\"]\n",
    "        A[(\"Data Lake (2ª Camada)\")]-- Query ---> B[\"Insight\"]\n",
    "    end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Criação da Tabela\n",
    "\n",
    "```sql\n",
    "CREATE EXTERNAL TABLE chatbot(\n",
    "  message_id bigint, \n",
    "  user_id bigint, \n",
    "  user_is_bot boolean, \n",
    "  user_first_name string, \n",
    "  chat_id bigint, \n",
    "  chat_type string, \n",
    "  text string, \n",
    "  date bigint)\n",
    "PARTITIONED BY ( \n",
    "  context_date date) -- informando que a tabela está particionada\n",
    "ROW FORMAT SERDE \n",
    "  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \n",
    "STORED AS INPUTFORMAT \n",
    "  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' \n",
    "OUTPUTFORMAT \n",
    "  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
    "LOCATION\n",
    "  's3://<Bucket>/' -- localização da 2ª camada do data lake\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Carregando os particionamentos\n",
    "\n",
    "```sql\n",
    "MSCK REPAIR TABLE chatbot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Algumas Queries para extrair informação dos dados coletados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quantidade de mensagens por dia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT \n",
    "  context_date, \n",
    "  count(1) AS \"message_amount\" \n",
    "FROM \"telegram\" \n",
    "GROUP BY context_date \n",
    "ORDER BY context_date DESC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quantidade de mensagens por usuário por dia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT \n",
    "  user_id, \n",
    "  user_first_name, \n",
    "  context_date, \n",
    "  count(1) AS \"message_amount\" \n",
    "FROM \"telegram\" \n",
    "GROUP BY \n",
    "  user_id, \n",
    "  user_first_name, \n",
    "  context_date \n",
    "ORDER BY context_date DESC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Média do tamanho das mensagens por usuário por dia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT \n",
    "  user_id, \n",
    "  user_first_name, \n",
    "  context_date,\n",
    "  CAST(AVG(length(text)) AS INT) AS \"average_message_length\" \n",
    "FROM \"telegram\" \n",
    "GROUP BY \n",
    "  user_id, \n",
    "  user_first_name, \n",
    "  context_date \n",
    "ORDER BY context_date DESC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quantidade de mensagens por hora por dia da semana por número da semana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "WITH \n",
    "parsed_date_cte AS (\n",
    "    SELECT \n",
    "        *, \n",
    "        CAST(date_format(from_unixtime(\"date\"),'%Y-%m-%d %H:%i:%s') AS timestamp) AS parsed_date\n",
    "    FROM \"telegram\" \n",
    "),\n",
    "hour_week_cte AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        EXTRACT(hour FROM parsed_date) AS parsed_date_hour,\n",
    "        EXTRACT(dow FROM parsed_date) AS parsed_date_weekday,\n",
    "        EXTRACT(week FROM parsed_date) AS parsed_date_weeknum\n",
    "    FROM parsed_date_cte\n",
    ")\n",
    "SELECT\n",
    "    parsed_date_hour,\n",
    "    parsed_date_weekday,\n",
    "    parsed_date_weeknum,\n",
    "    count(1) AS \"message_amount\" \n",
    "FROM hour_week_cte\n",
    "GROUP BY\n",
    "    parsed_date_hour,\n",
    "    parsed_date_weekday,\n",
    "    parsed_date_weeknum\n",
    "ORDER BY\n",
    "    parsed_date_weeknum,\n",
    "    parsed_date_weekday\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
